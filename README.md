# Finans SektÃ¶rÃ¼ Ä°Ã§in BÃ¼yÃ¼k Veri Boru HatlarÄ± Projesi ğŸ¦

*Bu proje, finans sektÃ¶rÃ¼nde karÅŸÄ±laÅŸÄ±lan yaygÄ±n senaryolarÄ± (EFT takibi, Veri AmbarÄ± modernizasyonu, mÃ¼ÅŸteri segmentasyonu) modelleyen, modern veri mÃ¼hendisliÄŸi araÃ§larÄ± ve best practice'ler kullanÄ±larak geliÅŸtirilmiÅŸ, uÃ§tan uca veri boru hatlarÄ± koleksiyonudur.*

-----

## 1\. Projenin Genel Felsefesi ve Best Practice'ler

Bu repo, tek bir uygulamadan Ã§ok, her biri kendi Ã¶zel problemini Ã§Ã¶zen, ancak ortak profesyonel prensipler Ã¼zerine inÅŸa edilmiÅŸ bir dizi veri mÃ¼hendisliÄŸi uygulamasÄ±nÄ± barÄ±ndÄ±rÄ±r. Proje genelinde aÅŸaÄŸÄ±daki yaklaÅŸÄ±mlar benimsenmiÅŸtir:

  * **ModÃ¼ler Mimari:** Proje, her biri kendi klasÃ¶rÃ¼nde yÃ¶netilen, baÄŸÄ±msÄ±z Ã§alÄ±ÅŸabilen alt uygulamalara (`eft_havale_app`, `dwh_app`, `odeme_kanali_app`) bÃ¶lÃ¼nmÃ¼ÅŸtÃ¼r.
  * **SorumluluklarÄ±n AyrÄ±lÄ±ÄŸÄ± (SoC):** Her alt uygulama, kendi iÃ§inde veri Ã¼retme (`producer`), veri iÅŸleme (`consumer`), konfigÃ¼rasyon (`configs`) ve yardÄ±mcÄ± araÃ§lar (`utils`) gibi mantÄ±ksal katmanlara ayrÄ±lmÄ±ÅŸtÄ±r.
  * **Ä°zole Ortamlar:** Her bir baÄŸÄ±msÄ±z bileÅŸen, kendi Python sanal ortamÄ±na (`venv`) ve `requirements.txt` dosyasÄ±na sahiptir. Bu, baÄŸÄ±mlÄ±lÄ±k Ã§akÄ±ÅŸmalarÄ±nÄ± Ã¶nler ve projenin taÅŸÄ±nabilirliÄŸini artÄ±rÄ±r.
  * **Merkezi KonfigÃ¼rasyon ve GÃ¼venlik:** Hassas bilgiler (ÅŸifreler, sunucu adresleri), `.env` dosyasÄ± aracÄ±lÄ±ÄŸÄ±yla koddan tamamen soyutlanmÄ±ÅŸtÄ±r ve `.gitignore` ile versiyon kontrolÃ¼ dÄ±ÅŸÄ±nda tutulmaktadÄ±r.
  * **SaÄŸlam (Robust) ve Hata ToleranslÄ± (Fault-Tolerant) Kod:** TÃ¼m dÄ±ÅŸ dÃ¼nya etkileÅŸimleri (API, veritabanÄ± baÄŸlantÄ±larÄ±, dosya iÅŸlemleri) `try-except-finally` bloklarÄ± ile gÃ¼vence altÄ±na alÄ±nmÄ±ÅŸtÄ±r. Spark Streaming iÅŸlerinde `checkpointing` mekanizmasÄ± kullanÄ±larak veri kaybÄ± Ã¶nlenmiÅŸtir.
  * **GÃ¼venli EriÅŸim:** Sunucudaki tÃ¼m servislere (MongoDB, Elasticsearch, Kibana, Spark UI) dÄ±ÅŸ aÄŸdan eriÅŸim kapatÄ±lmÄ±ÅŸ, tÃ¼m geliÅŸtirme ve yÃ¶netim iÅŸlemleri gÃ¼venli bir **SSH TÃ¼neli** Ã¼zerinden saÄŸlanmÄ±ÅŸtÄ±r.

## 2\. KullanÄ±lan Ana Teknolojiler

  * **Veri AkÄ±ÅŸÄ± ve MesajlaÅŸma:** Apache Kafka (KRaft Modu)
  * **Veri Ä°ÅŸleme:** Apache Spark 3.5.1 (PySpark - Batch & Structured Streaming)
  * **Veri Depolama:**
      * MongoDB (NoSQL - Analiz sonuÃ§larÄ± ve esnek veriler iÃ§in)
      * PostgreSQL (RDB - Geleneksel veri kaynaklarÄ±nÄ± simÃ¼le etmek iÃ§in)
  * **Veri Ãœretimi ve Entegrasyon:** Python, Faker
  * **AltyapÄ± ve YÃ¶netim:** DigitalOcean Droplet (Ubuntu - Single Node), `systemd` (Servis yÃ¶netimi)
  * **GÃ¶zlemlenebilirlik:** Python `logging` modÃ¼lÃ¼

## 3\. Proje Dosya YapÄ±sÄ±

```
banking-big-data-app/
â”œâ”€â”€ configs/
â”‚   â””â”€â”€ settings.py               # .env'den ayarlarÄ± okuyan, TÃœM uygulamalar iÃ§in ortak modÃ¼l
â”œâ”€â”€ utils/
â”‚   â””â”€â”€ logger.py                 # TÃœM uygulamalar iÃ§in merkezi loglama modÃ¼lÃ¼
â”‚
â”œâ”€â”€ eft_havale_app/               # UÃ§tan uca veri Ã¼retimi ve analizi uygulamasÄ±
â”‚   â”œâ”€â”€ producer/
â”‚   â””â”€â”€ consumer/
â”‚   â””â”€â”€ logs/
â”‚
â”œâ”€â”€ dwh_app/                      # RDB'den NoSQL'e ETL boru hattÄ± uygulamasÄ±
â”‚   â”œâ”€â”€ producer/
â”‚   â””â”€â”€ consumer/
â”‚   â””â”€â”€ logs/
â”‚
â”œâ”€â”€ odeme_kanali_app/             # MÃ¼ÅŸteri segmentasyon analizi uygulamasÄ±
â”‚   â”œâ”€â”€ producer/
â”‚   â””â”€â”€ consumer/
â”‚   â””â”€â”€ logs/
â”‚
â”œâ”€â”€ scripts/                      # spark-submit gibi komutlarÄ± iÃ§eren .sh script'leri
â”‚   â””â”€â”€ run_dwh_job.sh
â”‚   â””â”€â”€ run_odeme_kanali_batch_analysis.sh
â”‚
â”œâ”€â”€ eft_havale_transactions.csv   # dwh_app iÃ§in Ã¼retilen Ã¶rnek CSV dosyasÄ±
â”œâ”€â”€ .env.example                  # .env dosyasÄ±nÄ±n nasÄ±l olmasÄ± gerektiÄŸini gÃ¶steren ÅŸablon
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md
```

## 4\. Alt Uygulamalar

### 4.1. `eft_havale_app` - GerÃ§ek ZamanlÄ± Ä°ÅŸlem SimÃ¼lasyonu ve Analizi

  * **AmaÃ§:** Bankalar arasÄ± EFT/Havale iÅŸlemlerini gerÃ§ekÃ§i bir ÅŸekilde simÃ¼le eden ve bu veriyi Kafka Ã¼zerinden Spark ile hem toplu (batch) hem de anlÄ±k (streaming) olarak analiz eden bir boru hattÄ±.
  * **Mimari:** `Python Producer -> Kafka -> Spark (Batch & Streaming) -> MongoDB`
  * **Ã–ne Ã‡Ä±kan Ã–zellikler:** `Faker` ile tutarlÄ± kullanÄ±cÄ± havuzlarÄ± oluÅŸturma, `pivot` ve `window` gibi geliÅŸmiÅŸ Spark analiz fonksiyonlarÄ±nÄ±n kullanÄ±mÄ±.

### 4.2. `dwh_app` - RDB'den NoSQL'e ETL Projesi

  * **AmaÃ§:** Geleneksel bir iliÅŸkisel veritabanÄ±nda (PostgreSQL) bulunan yapÄ±sal veriyi, Spark kullanarak okumak, dÃ¶nÃ¼ÅŸtÃ¼rmek ve modern bir NoSQL veritabanÄ±na (MongoDB) yÃ¼klemek. Klasik bir Veri AmbarÄ± (DWH) modernizasyon senaryosunu temsil eder.
  * **Mimari:** `PostgreSQL (RDB) -> Spark Batch -> MongoDB (NoSQL)`
  * **Ã–ne Ã‡Ä±kan Ã–zellikler:** Spark'Ä±n JDBC veri kaynaÄŸÄ±nÄ± kullanarak iliÅŸkisel veritabanlarÄ±na baÄŸlanmasÄ± ve `DataFrameWriter` API'si ile NoSQL hedeflerine veri yazmasÄ±.

### 4.3. `odeme_kanali_app` - MÃ¼ÅŸteri Segmentasyon Analizi

  * **AmaÃ§:** Kredi kartÄ± kullanÄ±m loglarÄ±nÄ± analiz ederek, bankanÄ±n modern Ã¶deme kanallarÄ±nÄ± (QRCode, Sanal Kart) kullanmayan ancak potansiyeli yÃ¼ksek olan mÃ¼ÅŸteri segmentlerini tespit etmek.
  * **Mimari:** `Python Producer -> Kafka -> Spark Batch -> MongoDB`
  * **Ã–ne Ã‡Ä±kan Ã–zellikler:** Ä°ÅŸ probleminden yola Ã§Ä±karak, `filter` ve `groupBy` gibi temel Spark operasyonlarÄ± ile hedefe yÃ¶nelik pazarlama listeleri oluÅŸturma.

## 5\. Kurulum ve Ã‡alÄ±ÅŸtÄ±rma

### Sunucu Gereksinimleri

Projenin Ã§alÄ±ÅŸtÄ±rÄ±lacaÄŸÄ± sunucuda Java, Python3, `venv`, `git` ve ana servislerin (Kafka, Spark, MongoDB, PostgreSQL) kurulu ve Ã§alÄ±ÅŸÄ±r durumda olmasÄ± gerekmektedir.

### Yerel Kurulum ve Ã‡alÄ±ÅŸtÄ±rma

1.  **Depoyu Klonlama:** `git clone <proje_url>`
2.  **.env DosyasÄ±:** Proje kÃ¶k dizininde `.env.example` dosyasÄ±nÄ± kopyalayarak `.env` adÄ±nda yeni bir dosya oluÅŸturun ve iÃ§indeki hassas bilgileri (ÅŸifreler, sunucu adresleri vb.) kendi ortamÄ±nÄ±za gÃ¶re doldurun.
3.  **BaÄŸÄ±mlÄ±lÄ±klar:** Her bir alt uygulamanÄ±n `producer` ve `consumer` klasÃ¶rlerine girerek, `venv` sanal ortamlarÄ±nÄ± oluÅŸturun ve `pip install -r requirements.txt` ile baÄŸÄ±mlÄ±lÄ±klarÄ± kurun.
4.  **Ã‡alÄ±ÅŸtÄ±rma:** Her uygulamanÄ±n kendi `README.md` dosyasÄ±nda veya `scripts/` klasÃ¶rÃ¼ndeki `.sh` dosyalarÄ±nda detaylÄ± Ã§alÄ±ÅŸtÄ±rma talimatlarÄ± bulunur. Genel akÄ±ÅŸ, `producer` script'ini Ã§alÄ±ÅŸtÄ±rarak Kafka'yÄ± beslemek ve ardÄ±ndan `consumer` script'ini `spark-submit` ile Ã§alÄ±ÅŸtÄ±rmaktÄ±r.